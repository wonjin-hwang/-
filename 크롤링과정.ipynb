{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install time\n",
    "%pip install pandas\n",
    "%pip install BeautifulSoup\n",
    "%pip install openpyxl\n",
    "%pip install requests\n",
    "%pip install openpyxl\n",
    "%pip install bs4\n",
    "%pip install pyOpenSSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수 (공무원+키워드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#키워드\n",
    "k = pd.read_csv('키워드.txt')\n",
    "keywords = k['키워드'].values.tolist()\n",
    "\n",
    "#지자체\n",
    "f = pd.read_csv('경기.txt')\n",
    "regions = f['경기'].values.tolist()\n",
    "\n",
    "#업무명\n",
    "w = pd.read_csv('업무명.txt')\n",
    "works = w['업무'].values.tolist()\n",
    "\n",
    "#사이트1\n",
    "site1 = pd.read_csv('경기변수1.txt', sep='\\t')\n",
    "site1 = site1.fillna('')\n",
    "\n",
    "#사이트2\n",
    "site2 = pd.read_csv('경기변수2.txt', sep='\\t')\n",
    "site2 = site2.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time 복잡ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "article_urls = []\n",
    "titles = []\n",
    "days = []\n",
    "content = []\n",
    "\n",
    "for j in range(len(site1)):\n",
    "    url = site1['url'][j]  #1\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    result = soup.find_all(site1['result1'][j], attrs = {site1['result2'][j] : site1['result3'][j]}) #2 #3 #4\n",
    "    if len(result) == 0:\n",
    "        article_urls.append('')\n",
    "        titles.append('')\n",
    "        days.append('')\n",
    "        content.append('')\n",
    "        names.append('')\n",
    "    else:\n",
    "        for i in range(len(result) - 1):\n",
    "            name = site1['신문사'][j]\n",
    "            names.append(name)\n",
    "            idx=result[i].find('a')['href'].find('/')\n",
    "            if idx == -1: idx = 0\n",
    "            article_url = site1['article_url'][j] + result[i].find('a')['href'][idx:] #5\n",
    "            article_urls.append(article_url)\n",
    "            response=requests.get(article_url)\n",
    "            response.encoding = site1['encoding'][j]  #6\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.find(site1['title1'][j], attrs = {site1['title2'][j] : site1['title3'][j]}).text  #7 #8 #9\n",
    "            titles.append(title)\n",
    "            time = soup.find(site1['time1'][j],attrs = {site1['time2'][j] : site1['time3'][j]}).find_all(site1['time4'][j])[site1['time5'][j]].text #10 #11 #12 #13 #14\n",
    "            #time = time[3:]\n",
    "            days.append(time)\n",
    "            article = soup.find_all(site1['article1'][j], attrs = {site1['article2'][j]:site1['article3'][j]}) #15\n",
    "            body = ''\n",
    "            for p in article:\n",
    "                body += p.text\n",
    "                body  += ' '\n",
    "            content.append(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### time 간단ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(site2)):\n",
    "    url = site2['url'][j]  #1\n",
    "    soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
    "    result = soup.find_all(site2['result1'][j], attrs = {site2['result2'][j] : site2['result3'][j]}) #2 #3 #4\n",
    "    if len(result) == 0:\n",
    "        article_urls.append('')\n",
    "        titles.append('')\n",
    "        days.append('')\n",
    "        content.append('')\n",
    "        names.append('')\n",
    "    else:\n",
    "        for i in range(len(result)-1):\n",
    "            name = site2['신문사'][j]\n",
    "            names.append(name)\n",
    "            idx=result[i].find('a')['href'].find('/')\n",
    "            if idx == -1: idx = 0\n",
    "            article_url = site2['article_url'][j] + result[i].find('a')['href'][idx:] #5\n",
    "            article_urls.append(article_url)\n",
    "            response=requests.get(article_url)\n",
    "            response.encoding = site2['encoding'][j]  #6\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.find(site2['title1'][j],attrs = {site2['title2'][j] : site2['title3'][j]}).text  #7 #8 #9\n",
    "            titles.append(title)\n",
    "            time = soup.find(site2['time1'][j],attrs = {site2['time2'][j] : site2['time3'][j]}).text #10 #11 #12\n",
    "            #time = time[3:]\n",
    "            days.append(time)\n",
    "            article = soup.find_all(site2['article1'][j], attrs = {site2['article2'][j]:site2['article3'][j]}) #13\n",
    "            body = ''\n",
    "            for p in article:\n",
    "                body += p.text\n",
    "                body  += ' '\n",
    "            content.append(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최후 DF 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "기사_경기 = pd.DataFrame({'신문사':names, '제목': titles, '날짜': days, 'url': article_urls, '본문': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 날짜 형식 맞추기\n",
    "for i in range(len(기사_경기)):\n",
    "    info = re.sub(r'[^0-9]', '', 기사_경기['날짜'][i])\n",
    "    기사_경기['날짜'][i] = info[0:8]\n",
    "\n",
    "#### 어제, 오늘 날짜인 기사만\n",
    "from datetime import datetime\n",
    "기사_경기_어제 = 기사_경기[(datetime.today() - timedelta(1)).strftime(\"%Y%m%d\") == 기사_경기['날짜']].reset_index(drop=True)\n",
    "기사_경기_오늘 = 기사_경기[기사_경기['날짜'] == datetime.today().strftime(\"%Y%m%d\")].reset_index(drop=True)\n",
    "기사_경기1 = pd.concat([기사_경기_어제, 기사_경기_오늘]).reset_index(drop=True)\n",
    "기사_경기2 = 기사_경기1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공무원 + 지역 + 기본 키워드\n",
    "\n",
    "for i, txt in enumerate(기사_경기1['본문']):\n",
    "    score = any(keyword in txt for keyword in keywords)+any(region in txt for region in regions)  #+any(work in txt for work in works)\n",
    "    if score != 2:\n",
    "        기사_경기1 = 기사_경기1.drop(index = i, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복되는 내용 지우기\n",
    "기사_경기1 = 기사_경기1.drop_duplicates(subset='본문', ignore_index = True)\n",
    "기사_경기1 = 기사_경기1.sort_values(by = '날짜', ascending = False)\n",
    "\n",
    "#기사_경기1['본문'] = [re.sub('[^A-Za-z0-9가-힣\\s]', '', s) for s in 기사_경기1['본문']]\n",
    "기사_경기1['본문'] = [re.sub(\"\\n\", \"\", s) for s in 기사_경기1['본문']]\n",
    "\n",
    "# 제거\n",
    "기사_경기1['본문'] = [s.replace(u'\\xa0', u'') for s in 기사_경기1['본문']]\n",
    "# 기사_경기1['본문'] = [s.replace(' ', '') for s in 기사_경기1['본문']]\n",
    "\n",
    "\n",
    "기사_경기_지역기본 = 기사_경기1.iloc[:,0:4]\n",
    "기사_경기_지역기본.to_excel('경기_지역+기본.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공무원 + 지역 + 업무 키워드\n",
    "\n",
    "for i, txt in enumerate(기사_경기2['본문']):\n",
    "    score = any(region in txt for region in regions) + any(work in txt for work in works)\n",
    "    if score != 2:\n",
    "        기사_경기2 = 기사_경기2.drop(index = i, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복되는 내용 지우기\n",
    "기사_경기2 = 기사_경기2.drop_duplicates(subset='본문', ignore_index = True)\n",
    "기사_경기2 = 기사_경기2.sort_values(by = '날짜', ascending = False)\n",
    "\n",
    "기사_경기2['본문'] = [re.sub('[^A-Za-z0-9가-힣\\s]', '', s) for s in 기사_경기2['본문']]\n",
    "기사_경기2['본문'] = [re.sub(\"\\n\", \"\", s) for s in 기사_경기2['본문']]\n",
    "\n",
    "# 제거\n",
    "기사_경기2['본문'] = [s.replace(u'\\xa0', u'') for s in 기사_경기2['본문']]\n",
    "# 기사_경기2['본문'] = [s.replace(' ', '') for s in 기사_경기2['본문']]\n",
    "\n",
    "        \n",
    "기사_경기_지역업무 = 기사_경기2.iloc[:,0:4]\n",
    "기사_경기_지역업무.to_excel('경기_지역+업무.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
